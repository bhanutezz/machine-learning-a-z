print(apple)
apple <- c('green','yellow','orange')
print(apple)
?read.table
list1=list(c(2,3,4),2.4,sin)
#print the list
print(list1)
library(datasets)
head(airquality)
s <- split(airquality,airquality$Month)
lapply(s,function(x) colMeans(x[,c("Ozone","Solar.R","Wind")]))
sapply(s,function(x) colMeans(x[,c("Ozone","Solar.R","Wind")]))
sapply(s,function(x) colMeans(x[,c("Ozone","Solar.R","Wind")],na.rm = TRUE))
library(stats)
?stats
library(XML)
fileURL <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileURL, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
xmlSApply(rootNode, xmlValue)
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)
fileURL <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc<- htmlTreeParse(fileURL, useInternal = TRUE)
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)
scores
teams
print(scores)
print(teams)
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
head(jsonData)
names(jsonData)
names(jsonData$owner)
jsonData$owner$login
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
iris2 <- fromJSON(myjson)
head(iris2)
head(iris)
iris
library(httr)
#get data
data1 <- fromJSON("https://api.github.com/users/hadley/orgs")
#it's a data frame
names(data1)
data1$login
data2 <- fromJSON("https://api.github.com/users/hadley/repos")
#it's a data frame...
names(data2)
data2$name
#...with has a nested data frame
names(data2$owner)
data2$owner$login
#these are equivalent :)
data2[1,]$owner$login
data2[1,"owner"]$login
data2$owner[1,"login"]
data2$owner[1,]$login
library(data.table)
DF = data.frame(x=rnorm(9), y= rep(c("a","b","c"), each=3), z=rnorm(9))
head(DF, 3)
DT = data.table(x=rnorm(9), y= rep(c("a","b","c"), each=3), z=rnorm(9))
head(DT, 3)
tables()
DT[2,]
DT[2,1]
DT[2,2]
DT[2,3]
DT[,3]
DT[,1]
tables()
DT[2,]
DT[,2]
DT[DT$y=="a",]
DT[2,]
tables()
head(DT, 3)
DT[DT$y=="a",]
DT[c(2,3)]
DT[,list(mean(x),sum(z))]
DT[,table(y)]
DT[, w:=z^2]
head(DT, 3)
DT2<-DT
DT[,y:=2]
head(DT,n=3)
head(DT2,n=3)
DT[,y:=a]
DT[,y:="a"]
head(DT,n=3)
head(DT2,n=3)
DT[,y:=2]
head(DT,n=3)
head(DT2,n=3)
DT[,m:={tmp<-(x+z); log2(tmp+5)}]
head(DT,n=3)
DT[,a:=x>0]
head(DT,n=3)
DT[,b:=mean(x+w),by=a]
head(Dt)
head(DT)
install.packages("swirl")
packageVersion("swirl")
library(swirl)
install_from_swirl("R Programming")
swirl()
5+7
install.packages("RMySQL", type="source")
install.packages("RMySQL", type="source", dependencies=TRUE, INSTALL_opts = c('--no-lock'))
library(httr)
myapp = oauth_app("twitterApp", key="	oKkhCHCA48IL9fHjQcnay62pm", secret = "DSFVMdWlytlQy2GX8Mqp5jAsbbdsVL5lYVL6YgSRTnzs13MXl1")
sig = sign_oauth1.0(myapp, token = "2853054823-i0gJ7OVKrF6OXsGvDjO9vMaYGwHNFh3G2Rtdy6d", token_secret = "gHdMZ5KyFVQmDRaBpEMsDXI6YwnP8smR1wn2twOwAUSzP")
nohomeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
library(rjson)
json1 = content(homeTL)
json1 = content(nohomeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1, 1:4]
print(json2)
set.seed(13435)
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
X <- X[sample(1:5),];X$var2[c(1,3)]=NA
X
X[,1]
X[,"var1"]
X[1:2, "var2"]
X[(X#var1 <= 3 & X$var3 >11),]
X[(X$var1 <= 3 & X$var3 >11),]
X[(X$var1 <= 3 & X$var3 >11),]
X[(X$var1 <= 3 | X$var3 >15),]
X[1,1]
X[which(X$var2>8),]
?which
sort(X$var1)
sort(X$var1)
sort(X$var1, decreasing = TRUE)
sort(X$var1, decreasing = true)
sort(X$var1, decreasing = TRUE)
sort(X$var2, na.last = TRUE)
sort(X$var2, na.last = FALSE)
sort(X$var2, na.last = NA)
sort(X$var2, na.last = TRUE)
X[order(X$var1),]
X[order(X$var2),]
X[order(X$var1, X$var3),]
library(plyr)
arrange(X, var1)
arrange(X, desc(var1))
##@Author Shivam Sharma(28shivamsharma@gmail.com)
library("stringr")
library("SnowballC")
library("openNLP")
library("NLP")
##Find total number of adjectives in datasets
adjectiveFinder <- function(File)
{
File <- as.String(File)
##Sentence annotator
sent_token <- Maxent_Sent_Token_Annotator()
##Word Annotator
word_token <- Maxent_Word_Token_Annotator()
##Part of speech tagger annotator
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
##Annotation without features
annotation1 <- annotate(File,list(sent_token, word_token))
##Second annotation with POS
annotation2 <- annotate(File, pos_tag_annotator, annotation1)
##Taking only word tokens
annotation_word <- subset(annotation2, type == "word")
tags <- sapply(annotation_word$features, `[[`, "POS")
temp <- sprintf("%s---%s",File[annotation_word],tags)
##Finding pattern ---JJ(Basically adjectives) in vector
temp <- temp[grep(pattern = "---JJ",temp,fixed = TRUE)]
##Replacement of ---JJ with no string
temp <- gsub(pattern = "---JJS|---JJR|---JJ",replacement = "",temp)
return(temp)
}
##Reading Datasets
train <- read.delim(file="E:\\R\\train.tsv\\train.tsv",sep="\t",header = TRUE )
test <- read.delim(file="E:\\R\\test.tsv\\test.tsv",sep = "\t",header = TRUE)
temp <- as.vector(character())
##Making suitable model
for(i in 1:8544)
{
temp <- c(temp,as.character(train[train$SentenceId==i,][1,3]))
}
temp1 <- as.vector(character())
for(i in 8545:11855)
{
temp1 <- c(temp1,as.character(test[test$SentenceId==i,][1,3]))
}
features <- as.vector(character())
for(i in 1:length(temp))
{
features <- c(features,adjectiveFinder(temp[i]))
#print(i)
}
features <- unique(features)
features <- c(features,adjectiveFinder(paste(temp,collapse = ' ')))
features <- c(features,adjectiveFinder(paste(temp1,collapse = ' ')))
?as.vector()
?charact
?character
v1 <- c(1,2,3)
all(v1)
any(v1)
v1[1:3]
v1[c(1,3)]
v1[c(2,3)]
v1[is.na(v1)]<-0
v1
names(v1) <- c('first','second','third')
v1
as.factor(v1)
v2<-c(1,2,3,4,3)
as.factor(v2)
as.factor(v2)
vector(mode = 'list', length = 3)
list1<-vector(mode = 'list', length = 3)
list1
list1 <- list(first='a', second='2')
list1
list1[[1]]
list1[[2]]<-'3'
list1
list1[[2]]<-3
list1
?list
getwd()
setwd("C:/machineLearning/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Data_Preprocessing")
getwd()
# Importing the dataset
dataset = read.csv('Data.csv')
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('Data.csv')
View(dataset)
View(dataset)
# Importing the dataset
dataset = read.csv('Data.csv')
View(dataset)
View(dataset)
?ave()
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
View(dataset)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
dataset$Country = factor(dataset$Country,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3))
View(dataset)
View(dataset)
dataset$Purchased = factor(dataset$Purchased,
levels = c('No', 'Yes'),
labels = c(0, 1))
# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
split
training_Set = subset(dataset, split == TRUE)
test_Set = subset(dataset, split == FALSE)
View(training_Set)
View(test_Set)
View(training_Set)
training_set = scale(training_set)
test_set = scale(test_set)
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('Data.csv')
# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Encoding categorical data
dataset$Country = factor(dataset$Country,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3))
dataset$Purchased = factor(dataset$Purchased,
levels = c('No', 'Yes'),
labels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$DependentVariable, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('Data.csv')
# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Encoding categorical data
dataset$Country = factor(dataset$Country,
levels = c('France', 'Spain', 'Germany'),
labels = c(1, 2, 3))
dataset$Purchased = factor(dataset$Purchased,
levels = c('No', 'Yes'),
labels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set = scale(training_set)
test_set = scale(test_set)
training_set[, 2:3] = scale(training_set)
training_set[, 2:3] = scale(training_set[, 2:3])
View(training_Set)
View(training_Set)
View(training_set)
View(test_set)
test_set[, 2:3] = scale(test_set[, 2:3])
View(test_set)
